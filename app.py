import os
import sys
import json
import requests
from flask import Flask, request, Response, stream_with_context, jsonify
from flask_cors import CORS
from langchain_community.vectorstores import Neo4jVector
from py2neo import Graph
from zhipuai import ZhipuAI
import re
import heapq
from collections import defaultdict

# åˆå§‹åŒ– Flask åº”ç”¨
app = Flask(__name__)
CORS(app)

# **ç¡®ä¿ BASE_DIR åœ¨æœ€å‰é¢å®šä¹‰**
if getattr(sys, 'frozen', False):  # æ£€æµ‹æ˜¯å¦ä¸º PyInstaller æ‰“åŒ…ç¯å¢ƒ
    BASE_DIR = sys._MEIPASS  # PyInstaller è§£å‹åçš„ä¸´æ—¶ç›®å½•
else:
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # æ™®é€š Python è¿è¡Œç¯å¢ƒ

# è¯»å–é…ç½®æ–‡ä»¶
CONFIG_FILE = os.path.join(BASE_DIR, "config.json")
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        config = json.load(f)
else:
    config = {}

# è®¾å®š input_data æ–‡ä»¶å¤¹è·¯å¾„
INPUT_DATA_FOLDER = os.path.join(BASE_DIR, "input_data")
if not os.path.exists(INPUT_DATA_FOLDER):
    os.makedirs(INPUT_DATA_FOLDER)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨

# è®¾ç½® OpenAI åå‘ä»£ç†
TARGET_OPENAI_PROXY = config.get("openai_proxy", "http://default-proxy-url.com")  # é»˜è®¤å€¼å¯è‡ªè¡Œä¿®æ”¹

# è¿æ¥åˆ° Neo4j æ•°æ®åº“
neo4j_config = config.get("neo4j", {})
NEO4J_URL = neo4j_config.get("url", "bolt://localhost:7687")
NEO4J_USERNAME = neo4j_config.get("username", "neo4j")
NEO4J_PASSWORD = neo4j_config.get("password", "neo4j")

graph = Graph(NEO4J_URL, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

# è¯»å–æ™ºè°± API Key
zhipu_config = config.get("zhipu", {})
ZHIPU_API_KEY = zhipu_config.get("api_key", "")

# åˆå§‹åŒ–æ™ºè°±å®¢æˆ·ç«¯
client = ZhipuAI(api_key=ZHIPU_API_KEY)

def flatten_properties(properties):
    """å°†åµŒå¥—å­—å…¸è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²"""
    for key, value in properties.items():
        if isinstance(value, dict):  # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²å­˜å‚¨
            properties[key] = json.dumps(value, ensure_ascii=False)
    return properties

# æ·»åŠ è‡ªåŠ¨åˆ›å»ºå‘é‡ç´¢å¼•çš„æ–¹æ³•
def ensure_vector_index():
    """ä¸ºæ‰€æœ‰é¢„å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹åˆ›å»ºä¸¤ä¸ª Neo4j å‘é‡ç´¢å¼•ï¼šbasic_embedding å’Œ full_embedding"""
    try:
        # **1. ä» config.json è¯»å–é¢„å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹**
        predefined_labels = config.get("predefined_labels", ["entity"])

        if not isinstance(predefined_labels, list) or not predefined_labels:
            print("âŒ é…ç½®æ–‡ä»¶ä¸­çš„ `predefined_labels` æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
            predefined_labels = ["school_uniform", "student", "teacher"]

        print(f"ğŸ“Œ é¢„å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹: {predefined_labels}")

        # **2. æ£€æŸ¥ GDS æ’ä»¶æ˜¯å¦å®‰è£…**
        gds_check_query = "SHOW PROCEDURES YIELD name WHERE name STARTS WITH 'gds.' RETURN name LIMIT 1"
        gds_installed = bool(graph.run(gds_check_query).data())

        if not gds_installed:
            print("âŒ Neo4j Graph Data Science (GDS) æ’ä»¶æœªå®‰è£…ï¼Œå‘é‡ç´¢å¼•å¯èƒ½æ— æ³•ä½¿ç”¨ï¼")
            return

        # **3. æ£€æŸ¥å·²æœ‰ç´¢å¼•**
        check_query = "SHOW INDEXES YIELD name"
        existing_indexes = [index["name"] for index in graph.run(check_query).data()]

        # **4. éå†æ‰€æœ‰é¢„å®šä¹‰çš„èŠ‚ç‚¹ç±»å‹ï¼Œåˆ›å»ºä¸¤ä¸ªç´¢å¼•**
        for label in predefined_labels:
            for embedding_type in ["basic", "full"]:
                index_name = f"vector_index_{label}_{embedding_type}"
                embedding_field = f"{embedding_type}_embedding"

                if index_name not in existing_indexes:
                    print(f"ğŸ” å‘é‡ç´¢å¼• `{index_name}` ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
                    create_index_query = f"""
                    CREATE VECTOR INDEX {index_name} FOR (n:`{label}`) ON (n.{embedding_field})
                    OPTIONS {{
                      indexConfig: {{
                        `vector.dimensions`: 2048,
                        `vector.similarity_function`: "cosine"
                      }}
                    }};
                    """
                    graph.run(create_index_query)
                    print(f"âœ… å‘é‡ç´¢å¼• `{index_name}` åˆ›å»ºæˆåŠŸï¼")
                else:
                    print(f"âœ… å‘é‡ç´¢å¼• `{index_name}` å·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»ºã€‚")

    except Exception as e:
        print(f"âŒ åˆ›å»ºå‘é‡ç´¢å¼•å¤±è´¥ï¼š{e}")

# åˆ é™¤å…¨éƒ¨å‘é‡ç´¢å¼•
@app.route('/neo4j/delete_vector_indexes', methods=['POST'])
def delete_all_vector_indexes_api():
    """æä¾› API ç«¯ç‚¹ä»¥åˆ é™¤ Neo4j æ‰€æœ‰å‘é‡ç´¢å¼•"""
    try:
        # **1. è·å–æ‰€æœ‰ç´¢å¼•**
        check_query = "SHOW INDEXES YIELD name"
        existing_indexes = [index["name"] for index in graph.run(check_query).data()]

        # **2. è¿‡æ»¤å‡ºæ‰€æœ‰ `vector_index_` å¼€å¤´çš„ç´¢å¼•**
        vector_indexes = [idx for idx in existing_indexes if idx.startswith("vector_index_")]

        if not vector_indexes:
            print("âš ï¸ æ²¡æœ‰å‘é‡ç´¢å¼•éœ€è¦åˆ é™¤ã€‚")
            return jsonify({"status": "warning", "message": "æ²¡æœ‰å‘é‡ç´¢å¼•éœ€è¦åˆ é™¤"}), 200

        # **3. é€ä¸ªåˆ é™¤ç´¢å¼•**
        for index_name in vector_indexes:
            print(f"ğŸ—‘ æ­£åœ¨åˆ é™¤å‘é‡ç´¢å¼• `{index_name}`...")
            drop_query = f"DROP INDEX {index_name}"
            graph.run(drop_query)
            print(f"âœ… å‘é‡ç´¢å¼• `{index_name}` å·²æˆåŠŸåˆ é™¤ï¼")

        return jsonify({"status": "success", "message": f"å·²åˆ é™¤ {len(vector_indexes)} ä¸ªå‘é‡ç´¢å¼•"}), 200

    except Exception as e:
        print(f"âŒ åˆ é™¤å‘é‡ç´¢å¼•å¤±è´¥ï¼š{e}")
        return jsonify({"status": "error", "message": str(e)}), 500

# å®šä¹‰ embedding è®¡ç®—æ–¹æ³•
class ZhipuAIEmbeddings:
    def embed_query(self, text):
        print(f"ğŸ§  è®¡ç®— embedding: {text}")
        try:
            API_URL = "https://open.bigmodel.cn/api/paas/v4/embeddings"
            HEADERS = {
                "Authorization": f"Bearer {ZHIPU_API_KEY}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": "embedding-3",
                "input": [text]  # å¿…é¡»æ˜¯åˆ—è¡¨
            }

            response = requests.post(API_URL, json=payload, headers=HEADERS)
            response_json = response.json()

            if "error" in response_json:
                print(f"âŒ æ™ºè°± API é”™è¯¯: {response_json['error']['message']}")
                return None

            # ç¡®ä¿è¿”å›çš„æ•°æ®æ ¼å¼æ­£ç¡®
            if "data" not in response_json or not isinstance(response_json["data"], list):
                print(f"âŒ API è¿”å›æ ¼å¼é”™è¯¯: {response_json}")
                return None

            embedding = response_json["data"][0]["embedding"]
            if not embedding:
                print(f"âŒ æ— æ³•æå–åµŒå…¥å‘é‡: {response_json}")
                return None

            print(f"âœ… æå–çš„ embedding é•¿åº¦: {len(embedding)}")
            return embedding
        except Exception as e:
            print(f"âŒ è®¡ç®— embedding å‡ºé”™: {e}")
            return None

embedding_model = ZhipuAIEmbeddings()

def read_file_content(file_path):
    """è¯»å– JSON æˆ– TXT æ–‡ä»¶çš„å†…å®¹ï¼Œå¹¶è¿”å›è§£æåçš„æ•°æ®"""
    try:
        if file_path.endswith(".json"):
            with open(file_path, "r", encoding="utf-8") as file:
                return json.load(file)
        elif file_path.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read().strip()
                return {"name": os.path.basename(file_path), "content": content} if not content.startswith("{") else json.loads(content)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

def reset_vector_index():
    """é‡æ–°è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„ basic_embedding å’Œ full_embedding"""
    try:
        # **1. åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹çš„ embedding**
        graph.run("MATCH (n) REMOVE n.basic_embedding, n.full_embedding")
        print("âœ… å·²åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹çš„å‘é‡æ•°æ®")

        # **2. é‡æ–°è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„ embedding**
        query_nodes = "MATCH (n) RETURN n.name AS name, labels(n) AS labels, properties(n) AS properties"
        nodes = graph.run(query_nodes).data()

        for node in nodes:
            name = node.get("name", "Unnamed")
            labels = ", ".join(node.get("labels", []))  # ä¾‹å¦‚ "Person, Character"
            properties = node.get("properties", {})

            # **3. è®¡ç®— basic_embedding**
            alias = properties.get("alias", "")
            tag = properties.get("tag", "")

            # å¦‚æœ alias å’Œ tag å­˜åœ¨ï¼Œåˆ™åŠ å…¥æ–‡æœ¬ï¼Œå¦åˆ™å¿½ç•¥
            basic_text_parts = [f"åç§°: {name}"]
            if alias:
                basic_text_parts.append(f"åˆ«å: {alias}")
            if tag:
                basic_text_parts.append(f"æ ‡ç­¾: {tag}")

            basic_text = ", ".join(basic_text_parts)
            basic_embedding = embedding_model.embed_query(basic_text)

            if basic_embedding is None:
                print(f"âš ï¸ æ— æ³•è®¡ç®— `{name}` çš„ basic_embeddingï¼Œè·³è¿‡ã€‚")
                continue

            # **4. è®¡ç®— full_embedding**
            filtered_properties = {k: v for k, v in properties.items() if k not in ["basic_embedding", "full_embedding"]}
            full_text = f"åç§°: {name}, ç±»å‹: {labels}, è¯¦ç»†ä¿¡æ¯: {json.dumps(filtered_properties, ensure_ascii=False)}"
            full_embedding = embedding_model.embed_query(full_text)

            if full_embedding is None:
                print(f"âš ï¸ æ— æ³•è®¡ç®— `{name}` çš„ full_embeddingï¼Œè·³è¿‡ã€‚")
                continue

            # **5. å­˜å…¥ Neo4j**
            update_query = """
            MATCH (n {name: $name})
            SET n.basic_embedding = $basic_embedding,
                n.full_embedding = $full_embedding
            """
            graph.run(update_query, name=name, basic_embedding=basic_embedding, full_embedding=full_embedding)

        print("âœ… å…¨é‡å‘é‡åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ å…¨é‡å‘é‡åŒ–å¤±è´¥: {e}")

def incremental_vectorize(node_names):
    """å¢é‡å‘é‡åŒ–æ–°æ·»åŠ çš„èŠ‚ç‚¹ï¼Œè®¡ç®— basic_embedding å’Œ full_embedding"""
    for name in node_names:
        # **1. æŸ¥è¯¢èŠ‚ç‚¹å±æ€§**
        query = """
        MATCH (n {name: $name})
        RETURN labels(n) AS labels, properties(n) AS properties
        """
        node_data = graph.run(query, name=name).data()

        if not node_data:
            print(f"âš ï¸ èŠ‚ç‚¹ `{name}` ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue

        labels = ", ".join(node_data[0].get("labels", []))
        properties = node_data[0].get("properties", {})

        # **2. è®¡ç®— basic_embedding**
        alias = properties.get("alias", "")
        tag = properties.get("tag", "")

        basic_text_parts = [f"åç§°: {name}"]
        if alias:
            basic_text_parts.append(f"åˆ«å: {alias}")
        if tag:
            basic_text_parts.append(f"æ ‡ç­¾: {tag}")

        basic_text = ", ".join(basic_text_parts)
        basic_embedding = embedding_model.embed_query(basic_text)

        if basic_embedding is None:
            print(f"âš ï¸ æ— æ³•è®¡ç®— `{name}` çš„ basic_embeddingï¼Œè·³è¿‡ã€‚")
            continue

        # **3. è®¡ç®— full_embedding**
        filtered_properties = {k: v for k, v in properties.items() if k not in ["basic_embedding", "full_embedding"]}
        full_text = f"åç§°: {name}, ç±»å‹: {labels}, è¯¦ç»†ä¿¡æ¯: {json.dumps(filtered_properties, ensure_ascii=False)}"
        full_embedding = embedding_model.embed_query(full_text)

        if full_embedding is None:
            print(f"âš ï¸ æ— æ³•è®¡ç®— `{name}` çš„ full_embeddingï¼Œè·³è¿‡ã€‚")
            continue

        # **4. æ›´æ–° Neo4j**
        update_query = """
        MATCH (n {name: $name})
        SET n.basic_embedding = $basic_embedding,
            n.full_embedding = $full_embedding
        """
        graph.run(update_query, name=name, basic_embedding=basic_embedding, full_embedding=full_embedding)

    print("âœ… å¢é‡å‘é‡åŒ–å®Œæˆ")

# åˆ›å»ºå…³ç³»
def create_relationships_for_nodes(node_names=None):
    """ä¸ºå¢é‡æˆ–å…¨é‡å¯¼å…¥çš„èŠ‚ç‚¹åˆ›å»ºå…³ç³»"""
    try:
        relationships = config.get("create_relationship", [])
        if not relationships:
            print("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰å®šä¹‰ä»»ä½•å…³ç³»è§„åˆ™ï¼Œè·³è¿‡åˆ›å»ºå…³ç³»ã€‚")
            return

        if not node_names:
            query_nodes = "MATCH (n) RETURN n.name AS name"
            node_names = [record["name"] for record in graph.run(query_nodes).data()]

        if not node_names:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„èŠ‚ç‚¹ï¼Œè·³è¿‡å…³ç³»åˆ›å»ºã€‚")
            return

        for rule in relationships:
            if len(rule) != 3:
                continue

            source_label, relationship_type, target_label = rule
            print(f"ğŸ”— å¤„ç†å…³ç³»: ({source_label}) -[:{relationship_type}]-> ({target_label})")

            # **åŠ¨æ€è·å–ç›®æ ‡å­—æ®µå**
            query_source = f"""
            MATCH (s:`{source_label}`)
            WHERE s.name IN $node_names
            RETURN s.name AS source_name, s.`{target_label.lower()}` AS target_names
            """
            source_nodes = graph.run(query_source, node_names=node_names).data()

            for source_node in source_nodes:
                source_name = source_node["source_name"]
                target_names = source_node.get("target_names")

                if not target_names:
                    continue

                # ç¡®ä¿ target_names æ˜¯åˆ—è¡¨
                if not isinstance(target_names, list):
                    target_names = [target_names]

                for target_name in target_names:
                    query_target = f"""
                    MATCH (t:`{target_label}`)
                    WHERE t.name = $target_name OR $target_name IN t.alias
                    RETURN t.name AS target_name
                    """
                    target_nodes = graph.run(query_target, target_name=target_name).data()

                    if not target_nodes:
                        print(f"âš ï¸ æœªæ‰¾åˆ° `{target_name}` çš„åŒ¹é…èŠ‚ç‚¹ï¼Œè·³è¿‡")
                        continue

                    for target_node in target_nodes:
                        create_relationship_query = f"""
                        MATCH (s:`{source_label}` {{name: $source_name}})
                        MATCH (t:`{target_label}` {{name: $target_name}})
                        MERGE (s)-[:`{relationship_type}`]->(t)
                        """
                        graph.run(create_relationship_query, source_name=source_name, target_name=target_node["target_name"])
                        print(f"âœ… å…³ç³»å·²åˆ›å»º: ({source_name}) -[:{relationship_type}]-> ({target_node['target_name']})")

    except Exception as e:
        print(f"âŒ å…³ç³»åˆ›å»ºå¤±è´¥: {e}")

def convert_json_for_neo4j(data):
    """
    éå† JSONï¼Œå°†æ‰€æœ‰å¯¹è±¡ {} å’Œæ•°ç»„ [] è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥é€‚åº” Neo4j å­˜å‚¨
    """
    if isinstance(data, dict):
        return {k: convert_json_for_neo4j(v) for k, v in data.items()}
    elif isinstance(data, list):
        return json.dumps(data, ensure_ascii=False)  # å°†åˆ—è¡¨è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
    else:
        return data  # å…¶ä»–ç±»å‹ï¼ˆå­—ç¬¦ä¸²ã€æ•°å­—ã€å¸ƒå°”å€¼ï¼‰ä¿æŒä¸å˜

# å…¨é‡å¯¼å…¥
@app.route('/neo4j/full_import', methods=['POST'])
def full_import():
    """å…¨é‡åˆ é™¤æ•°æ®åé‡æ–°å¯¼å…¥ï¼Œå¹¶åˆ›å»ºå…³ç³»ã€é‡æ–°è®¡ç®—å‘é‡"""
    try:
        # **1. åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»**
        print("ğŸ—‘ åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»...")
        graph.run("MATCH (n) DETACH DELETE n")
        print("âœ… æ‰€æœ‰æ•°æ®å·²åˆ é™¤")

        # **2. è¯»å– `input_data` ç›®å½•ä¸­çš„æ‰€æœ‰ JSON/TXT æ–‡ä»¶**
        if not os.path.exists(INPUT_DATA_FOLDER):
            return jsonify({"status": "error", "message": "input_data æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 400

        results, new_nodes = [], []

        for filename in os.listdir(INPUT_DATA_FOLDER):
            if filename.endswith((".json", ".txt")):
                file_path = os.path.join(INPUT_DATA_FOLDER, filename)
                data = read_file_content(file_path)
                if not data or "name" not in data:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶: {filename}")
                    continue

                node_name = data["name"]
                label = data.get("type", "GenericEntity")  # é»˜è®¤æ ‡ç­¾
                # **è½¬æ¢ JSON ç»“æ„**
                properties = convert_json_for_neo4j(data)

                # **åˆ›å»ºèŠ‚ç‚¹**
                properties = flatten_properties(properties)
                create_query = f"CREATE (n:`{label}` {{ {', '.join(f'{k}: ${k}' for k in properties)} }}) RETURN n"
                result = graph.run(create_query, **properties).data()
                results.append({"file": filename, "created_node": result})
                new_nodes.append(node_name)

        print(f"âœ… å·²å¯¼å…¥ {len(new_nodes)} ä¸ªæ–°èŠ‚ç‚¹")

        # **3. åˆ›å»ºæ‰€æœ‰å…³ç³»**
        create_relationships_for_nodes()
        print("âœ… å·²åˆ›å»ºæ‰€æœ‰å…³ç³»")

        # **4. åˆ é™¤æ‰€æœ‰å‘é‡æ•°æ®**
        print("ğŸ—‘ åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹çš„å‘é‡æ•°æ®...")
        graph.run("MATCH (n) REMOVE n.embedding")
        print("âœ… å·²åˆ é™¤æ‰€æœ‰å‘é‡æ•°æ®")

        # **5. é‡æ–°è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„å‘é‡**
        reset_vector_index()
        print("âœ… å…¨é‡å‘é‡åŒ–å®Œæˆ")

        return jsonify({"status": "completed", "deleted_old": True, "new_nodes": new_nodes, "details": results})

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)})

@app.route('/neo4j/incremental_import', methods=['POST'])
def incremental_import():
    """å¢é‡å¯¼å…¥æ•°æ®ï¼Œå¹¶è‡ªåŠ¨åˆ›å»ºå…³ç³»å’Œå‘é‡åŒ–"""
    if not os.path.exists(INPUT_DATA_FOLDER):
        return jsonify({"status": "error", "message": "input_data æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}), 400

    results, new_nodes = [], []

    for filename in os.listdir(INPUT_DATA_FOLDER):
        if filename.endswith((".json", ".txt")):
            file_path = os.path.join(INPUT_DATA_FOLDER, filename)
            data = read_file_content(file_path)
            if not data or "name" not in data:
                continue

            node_name = data["name"]
            count = graph.run("MATCH (n {name: $name}) RETURN COUNT(n) AS count", name=node_name).evaluate()

            if count == 0:  # **å¦‚æœæ•°æ®åº“ä¸­ä¸å­˜åœ¨è¯¥èŠ‚ç‚¹ï¼Œåˆ™åˆ›å»º**
                label = data.get("type", "GenericEntity")
                # **è½¬æ¢ JSON ç»“æ„**
                properties = convert_json_for_neo4j(data)

                properties = flatten_properties(properties)
                create_query = f"CREATE (n:`{label}` {{ {', '.join(f'{k}: ${k}' for k in properties)} }}) RETURN n"
                result = graph.run(create_query, **properties).data()
                results.append({"file": filename, "created_node": result})
                new_nodes.append(node_name)

    print(f"âœ… å¢é‡å¯¼å…¥äº† {len(new_nodes)} ä¸ªæ–°èŠ‚ç‚¹")

    # **2. ä¸ºæ–°å¢çš„èŠ‚ç‚¹åˆ›å»ºå…³ç³»**
    if new_nodes:
        create_relationships_for_nodes(new_nodes)
        print("âœ… å·²åˆ›å»ºæ–°å¢èŠ‚ç‚¹çš„å…³ç³»")

        # **3. ä¸ºæ–°å¢çš„èŠ‚ç‚¹è¿›è¡Œå¢é‡å‘é‡åŒ–**
        incremental_vectorize(new_nodes)
        print("âœ… å·²å®Œæˆæ–°å¢èŠ‚ç‚¹çš„å‘é‡åŒ–")

    return jsonify({"status": "completed", "new_nodes": new_nodes, "details": results})

@app.route('/neo4j/import_file', methods=['POST'])
def import_file():
    """æ ¹æ®è¯·æ±‚çš„æ–‡ä»¶åå¯¼å…¥æˆ–æ›´æ–°æ•°æ®"""
    try:
        data = request.get_json()
        filename = data.get("filename")

        if not filename:
            return jsonify({"status": "error", "message": "ç¼ºå°‘å‚æ•° `filename`"}), 400

        file_path = os.path.join(INPUT_DATA_FOLDER, filename)

        if not os.path.exists(file_path):
            return jsonify({"status": "error", "message": f"æ–‡ä»¶ `{filename}` ä¸å­˜åœ¨"}), 404

        # **1. è¯»å–æ–‡ä»¶å†…å®¹**
        file_data = read_file_content(file_path)
        if not file_data or "name" not in file_data:
            return jsonify({"status": "error", "message": f"æ–‡ä»¶ `{filename}` æ ¼å¼æ— æ•ˆ"}), 400

        node_name = file_data["name"]
        label = file_data.get("type", "GenericEntity")  # é»˜è®¤æ ‡ç­¾
        properties = {k: v for k, v in file_data.items() if k != "type"}

        # **2. å±•å¹³å±æ€§ï¼Œé˜²æ­¢åµŒå¥—å­—å…¸é”™è¯¯**
        properties = flatten_properties(properties)

        # **3. æ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨**
        existing_count = graph.run("MATCH (n {name: $name}) RETURN COUNT(n) AS count", name=node_name).evaluate()

        if existing_count > 0:
            # **æ›´æ–°å·²æœ‰èŠ‚ç‚¹**
            update_query = f"""
            MATCH (n:`{label}` {{name: $name}})
            SET {', '.join(f'n.{k} = ${k}' for k in properties)}
            RETURN n
            """
            result = graph.run(update_query, name=node_name, **properties).data()
            action = "updated"
        else:
            # **åˆ›å»ºæ–°èŠ‚ç‚¹**
            create_query = f"""
            CREATE (n:`{label}` {{ {', '.join(f'{k}: ${k}' for k in properties)} }})
            RETURN n
            """
            result = graph.run(create_query, **properties).data()
            action = "created"

        # **4. åˆ›å»ºå…³ç³»**
        create_relationships_for_nodes([node_name])

        # **5. è®¡ç®—å‘é‡**
        incremental_vectorize([node_name])

        return jsonify({
            "status": "success",
            "action": action,
            "node": node_name,
            "details": result
        })

    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route('/neo4j/delete_all', methods=['POST'])
def delete_all_nodes():
    """åˆ é™¤ Neo4j æ•°æ®åº“ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹åŠå…¶å…³ç³»"""
    try:
        graph.run("MATCH (n) DETACH DELETE n")
        return jsonify({"status": "success", "message": "All nodes and relationships deleted."}), 200
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def split_into_sentences(text):
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­ï¼Œæ”¯æŒä¸­ã€è‹±ã€æ—¥"""
    sentence_endings = r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*'  # é€‚ç”¨äºä¸­ã€è‹±ã€æ—¥çš„å¥å·ã€é—®å·ã€æ„Ÿå¹å·
    sentences = re.split(sentence_endings, text)
    return [s.strip() for s in sentences if s.strip()]

def extract_query_message(messages):
    """æå– `<query_message>` å¹¶æ‹¼æ¥å†…å®¹"""
    query_text = ""
    in_query_message = False

    for msg in messages:
        content = msg["content"]

        if "<query_message>" in content:
            in_query_message = True
            query_text = content.split("<query_message>")[-1]  # å– <query_message> ååŠéƒ¨åˆ†

        elif "</query_message>" in content:
            query_text += "\n" + content.split("</query_message>")[0]  # å– </query_message> å‰åŠéƒ¨åˆ†
            in_query_message = False
            break  # ç»“æŸæ‹¼æ¥

        elif in_query_message:
            query_text += "\n" + content  # ç»§ç»­æ‹¼æ¥

    if query_text:
        query_text = f"<query_message>{query_text}</query_message>"

    return query_text if query_text else None

def extract_clean_query_message(query_message):
    """å»é™¤ `<query_message>` æ ‡ç­¾ï¼Œè¿”å›çº¯æ–‡æœ¬"""
    return query_message.replace("<query_message>", "").replace("</query_message>", "").strip()

def find_related_nodes_by_sentences(query_message):
    """å¯¹ query_message è¿›è¡Œæ¸…ç†ååˆ†å¥ï¼Œå¹¶åˆ†åˆ«åŸºäº basic_embedding å’Œ full_embedding æŸ¥è¯¢æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹"""
    clean_query = extract_clean_query_message(query_message)  # å»æ‰ <query_message> æ ‡ç­¾
    sentences = split_into_sentences(clean_query)  # åˆ†å¥
    print(f"ğŸ“ åˆ†å¥ç»“æœ: {sentences}")

    node_scores = defaultdict(float)

    # **æŸ¥è¯¢æ‰€æœ‰ `vector_index_*_basic` å’Œ `vector_index_*_full` ç´¢å¼•**
    check_query = "SHOW INDEXES YIELD name"
    existing_indexes = [index["name"] for index in graph.run(check_query).data()]
    vector_indexes_basic = [idx for idx in existing_indexes if idx.endswith("_basic")]
    vector_indexes_full = [idx for idx in existing_indexes if idx.endswith("_full")]

    for sentence in sentences:
        query_embedding = embedding_model.embed_query(sentence)
        if query_embedding is None:
            print(f"âš ï¸ æ— æ³•è®¡ç®— `{sentence}` çš„ embeddingï¼Œè·³è¿‡ã€‚")
            continue

        # **å­˜å‚¨å½“å‰å¥å­çš„æœ€å¤§ç›¸ä¼¼åº¦**
        sentence_max_scores = {}

        # **æŸ¥è¯¢ `basic_embedding` ç›¸ä¼¼åº¦**
        for index_name in vector_indexes_basic:
            query = f"""
            CALL db.index.vector.queryNodes('{index_name}', 10, $embedding)
            YIELD node, score
            RETURN node.name AS name, labels(node) AS labels, score
            """
            result = graph.run(query, embedding=query_embedding).data()

            for record in result:
                node_key = (record["name"], tuple(record["labels"]))
                sentence_max_scores[node_key] = max(sentence_max_scores.get(node_key, 0), record["score"])

        # **æŸ¥è¯¢ `full_embedding` ç›¸ä¼¼åº¦**
        for index_name in vector_indexes_full:
            query = f"""
            CALL db.index.vector.queryNodes('{index_name}', 10, $embedding)
            YIELD node, score
            RETURN node.name AS name, labels(node) AS labels, score
            """
            result = graph.run(query, embedding=query_embedding).data()

            for record in result:
                node_key = (record["name"], tuple(record["labels"]))
                sentence_max_scores[node_key] = max(sentence_max_scores.get(node_key, 0), record["score"])

        # **ç­›é€‰ç›¸ä¼¼åº¦ >= 0.6 çš„èŠ‚ç‚¹**
        filtered_nodes = {k: v for k, v in sentence_max_scores.items() if v >= 0.6}

        # **å–å½“å‰å¥å­çš„å‰ä¸‰ä¸ªæœ€ç›¸ä¼¼èŠ‚ç‚¹**
        top_sentence_nodes = heapq.nlargest(3, filtered_nodes.items(), key=lambda x: x)

        # **ç´¯åŠ å¾—åˆ†**
        for (node_key, score) in top_sentence_nodes:
            node_scores[node_key] += score

    # **æœ€ç»ˆå–å…¨å±€å¾—åˆ†æœ€é«˜çš„å‰ä¸‰ä¸ªèŠ‚ç‚¹**
    top_nodes = heapq.nlargest(3, node_scores.items(), key=lambda x: x)
    top_nodes = [{"name": name, "labels": labels} for (name, labels), _ in top_nodes]

    print(f"ğŸ¯ é€‰å‡ºçš„å‰ä¸‰ä¸ªç›¸å…³èŠ‚ç‚¹: {top_nodes}")

    return top_nodes

def get_relationship_types():
    """ä» Neo4j æŸ¥è¯¢æ‰€æœ‰å”¯ä¸€çš„å…³ç³»ç±»å‹"""
    try:
        query = "MATCH ()-[r]->() RETURN DISTINCT type(r) AS relationship_type"
        result = graph.run(query).data()

        # æå–æ‰€æœ‰å…³ç³»ç±»å‹å¹¶å»é‡
        relationship_types = sorted(set(record["relationship_type"] for record in result if record["relationship_type"]))

        print(f"ğŸ“Œ å‘ç°çš„å…³ç³»ç±»å‹: {relationship_types}")
        return relationship_types

    except Exception as e:
        print(f"âŒ è·å–å…³ç³»ç±»å‹å¤±è´¥: {e}")
        return []

def get_combined_node_content(nodes):
    """
    è·å–å¤šä¸ªç›¸å…³èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶å¤–æ‰©ä¸€å±‚è·å–å…¶ç›´æ¥å…³è”çš„èŠ‚ç‚¹ï¼Œå»é™¤ embedding å­—æ®µã€‚
    """
    content_parts = []
    processed_nodes = set()  # è®°å½•å·²å¤„ç†çš„èŠ‚ç‚¹ï¼Œé¿å…é‡å¤

    # **1. è·å–æ‰€æœ‰å…³ç³»ç±»å‹**
    relationship_types = get_relationship_types()
    if not relationship_types:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…³ç³»ç±»å‹ï¼Œè·³è¿‡å¤–æ‰©æŸ¥è¯¢ã€‚")
        relationship_filter = ""
    else:
        relationship_filter = "|:".join(f"`{rel}`" for rel in relationship_types)  # æ„é€  `|:` è¿æ¥çš„å…³ç³»å­—ç¬¦ä¸²

    for node in nodes:
        node_name = node.get("name", "æœªçŸ¥èŠ‚ç‚¹")

        # **2. è·å–å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰å±æ€§**
        query = """
        MATCH (n {name: $name})
        RETURN properties(n) AS props
        """
        result = graph.run(query, name=node_name).data()

        if not result:
            continue

        properties = result[0]["props"]

        # **3. è¿‡æ»¤æ‰ `basic_embedding` å’Œ `full_embedding`**
        properties = {k: v for k, v in properties.items() if k not in ["basic_embedding", "full_embedding"]}

        # **4. æ ¼å¼åŒ–å½“å‰èŠ‚ç‚¹å†…å®¹**
        content = f"ğŸ” [RAG] èµ„æ–™åº“è‡ªåŠ¨æå–: {node_name}"
        for key, value in properties.items():
            content += f"\n- {key}: {value}"

        content_parts.append(content.strip())
        processed_nodes.add(node_name)  # æ ‡è®°å½“å‰èŠ‚ç‚¹å·²å¤„ç†

        # **5. æŸ¥è¯¢å½“å‰èŠ‚ç‚¹çš„ç›´æ¥å…³è”èŠ‚ç‚¹**
        if relationship_filter:
            query = f"""
            MATCH (n {{name: $name}})-[:{relationship_filter}*1]-(related)
            RETURN DISTINCT related.name AS related_name, properties(related) AS related_props
            """
            related_nodes = graph.run(query, name=node_name).data()

            for related_node in related_nodes:
                related_name = related_node["related_name"]
                if related_name in processed_nodes:
                    continue  # é¿å…é‡å¤æ·»åŠ 

                related_props = {k: v for k, v in related_node["related_props"].items() if k not in ["basic_embedding", "full_embedding"]}

                # **6. æ ¼å¼åŒ–å…³è”èŠ‚ç‚¹å†…å®¹**
                related_content = f"ğŸ”— å…³è”èŠ‚ç‚¹: {related_name}"
                for key, value in related_props.items():
                    related_content += f"\n- {key}: {value}"

                content_parts.append(related_content.strip())
                processed_nodes.add(related_name)  # æ ‡è®°å…³è”èŠ‚ç‚¹å·²å¤„ç†

    return "\n\n".join(content_parts)


def replace_rag_data(messages, combined_content):
    """
    éå†èŠå¤©è®°å½•ï¼Œæ›¿æ¢ `[RAG_data]` æ ‡è®°ï¼Œç¡®ä¿ `embedding` ä¸ä¼šè¢«å‘é€ç»™ AIã€‚
    """
    updated_messages = []
    for msg in messages:
        if "[RAG_data]" in msg["content"]:
            msg["content"] = msg["content"].replace("[RAG_data]", combined_content)
            print(f"âœ… `[RAG_data]` å·²æ›¿æ¢: {msg['content']}")

        updated_messages.append(msg)

    return updated_messages

@app.route('/v1/<path:endpoint>', methods=['POST', 'GET'])
def proxy_request_with_rag(endpoint):
    """ä»£ç† OpenAI å…¼å®¹ API è¯·æ±‚ï¼Œå¹¶æŸ¥æ‰¾ `<query_message>` ç›¸å…³çš„æœ€ç›¸ä¼¼èŠ‚ç‚¹ï¼Œå¢å¼ºèŠå¤©æ•°æ®"""
    try:
        # **1. è¯»å– OpenAI ä»£ç† URL**
        openai_proxy = config.get("openai_proxy", "http://default-openai-proxy.com")

        # **2. è·å–å‰ç«¯ä¼ å…¥çš„ API Key**
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            return jsonify({"status": "error", "message": "ç¼ºå°‘ API Key"}), 401

        # **3. å¤„ç† `GET /v1/models` è¯·æ±‚**
        if request.method == "GET" and endpoint == "models":
            url = f"{openai_proxy}/v1/models"
            headers = {"Authorization": auth_header}

            response = requests.get(url, headers=headers)
            return Response(response.content, status=response.status_code, content_type=response.headers.get('Content-Type', 'application/json'))

        # **4. å¤„ç†èŠå¤©è¯·æ±‚**
        if endpoint == "chat/completions" and request.method == 'POST':
            data = request.get_json()
            messages = data.get("messages", [])

            if not messages:
                return jsonify({"status": "error", "message": "èŠå¤©è®°å½•ä¸ºç©º"}), 400

            # **æå– `<query_message>` å¹¶æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹**
            query_message = extract_query_message(messages)
            related_nodes = None  # ç”¨äºå­˜å‚¨æŸ¥è¯¢åˆ°çš„ç›¸å…³èŠ‚ç‚¹

            if query_message:
                print(f"\nğŸ” **æå–çš„ `<query_message>` å†…å®¹:**\n{query_message}\n")
                related_nodes = find_related_nodes_by_sentences(query_message)  # **æ›¿æ¢ `find_related_nodes`**

            # **å¦‚æœæ‰¾åˆ° `[RAG_data]`ï¼Œæ›¿æ¢ä¸ºæœ€ç›¸ä¼¼èŠ‚ç‚¹çš„å†…å®¹**
            if related_nodes:
                combined_content = get_combined_node_content(related_nodes)
                messages = replace_rag_data(messages, combined_content)

            # **7. è½¬å‘è¯·æ±‚åˆ° OpenAI ä»£ç†**
            url = f"{openai_proxy}/v1/chat/completions"
            headers = {key: value for key, value in request.headers.items() if key.lower() != 'host'}
            headers["Authorization"] = auth_header  # **ä½¿ç”¨å‰ç«¯ä¼ å…¥çš„ API Key**

            is_stream = data.get("stream", False)

            if is_stream:
                response = requests.post(url, json={"messages": messages, **data}, headers=headers, stream=True)
                return Response(stream_with_context(response.iter_content(chunk_size=1024)), content_type=response.headers['Content-Type'])
            else:
                response = requests.post(url, json={"messages": messages, **data}, headers=headers)
                return Response(response.content, status=response.status_code, content_type=response.headers['Content-Type'])

        # **8. å¤„ç†å…¶ä»– OpenAI å…¼å®¹ API è¯·æ±‚**
        url = f"{openai_proxy}/v1/{endpoint}"
        headers = {"Authorization": auth_header}

        if request.method == "GET":
            response = requests.get(url, headers=headers)
        else:
            response = requests.request(request.method, url, headers=headers, json=request.get_json())

        return Response(response.content, status=response.status_code, content_type=response.headers.get('Content-Type', 'application/json'))

    except Exception as e:
        print(f"âŒ ä»£ç†è¯·æ±‚å¤±è´¥: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500

flask_config = config.get("flask", {})
FLASK_PORT = flask_config.get("port", 8081)  # é»˜è®¤ç«¯å£ 8081
FLASK_DEBUG = flask_config.get("debug", False)  # é»˜è®¤å…³é—­ debug æ¨¡å¼

if __name__ == '__main__':
    # ç¡®ä¿ Neo4j å‘é‡ç´¢å¼•å·²åˆ›å»º
    print("ğŸ” ç¡®ä¿ Neo4j å‘é‡ç´¢å¼•å·²åˆ›å»º...")
    ensure_vector_index()

    # å¯åŠ¨ Flask æœåŠ¡å™¨
    print(f"ğŸš€ å¯åŠ¨ Flask æœåŠ¡å™¨ï¼Œç›‘å¬ç«¯å£ {FLASK_PORT}ï¼ŒDebug æ¨¡å¼: {FLASK_DEBUG}")
    app.run(host='0.0.0.0', port=FLASK_PORT, debug=FLASK_DEBUG)
